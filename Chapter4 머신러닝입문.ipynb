{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4장 머신러닝 입문\n",
    "\n",
    "## 머신러닝의 세가지 유형\n",
    "\n",
    "1. 지도학습 \n",
    "\n",
    "분류문제,\n",
    "회귀문제,\n",
    "이미지분할,\n",
    "음성인식,\n",
    "언어번역\n",
    "\n",
    "2. 비지도학습\n",
    "\n",
    "**클러스터링** : 유사한 데이터 그룹화\n",
    "**차원축소** : 차원축소\n",
    "3. 강화학습 : 요즘 주목받고 있는 머신러닝분야\n",
    "\n",
    "## 머신러닝 용어\n",
    "\n",
    "1. sample, input, data point : 학습 데이터에서 1개 데이터인 특정 인스턴스를 의미\n",
    "2. prediction, output : 출력값\n",
    "3. target, label : 각 sample에 명시된 목표값. 지도학습에서는 답이나 결과를 의미\n",
    "4. loss value, prediction error : 예측값과 실제 값의 차이를 측정하는 결과\n",
    "5. class : 주어진 데이터셋에 설정된 레이블의 집합. (예를 들어 고양이와 개의 클래스)\n",
    "6. binary classification(이진분류) \n",
    "7. Multi_class classification (다중 클래스 분류) : 클래스가 3개 이상의 범주로 구분되는 경우\n",
    "8. Multi-label classification (다중 레이블 분류) : 1개의 입력 데이터는 여러개의 레이블을 가질 수 있다. 예를 들어 퓨전 음식점에 경우 Italian Mexican Indian과 같이 여러 레이블이 될 수 있다. \n",
    "9. scaler regression(스칼라 회귀) : 입력 데이터가 1개의 스칼라인 것 (주식 예측 같은)\n",
    "10. vector regression(벡터 회귀) : 알고리즘이 2개 이상의 스칼라를 예측하는 상황에 해당. \n",
    "11. batch : 입력 데이터의 묶음. 각 배치별로 가중치가 업데이트 되기 때문에 단일 데이터보다 학습 속도가 빠른 경향을 보임.\n",
    "12. epoch : 학습 데이터셋을 전부 알고리즘에 학습시킨 단위를 epoch이라고 함. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 머신러닝 모델 평가\n",
    "\n",
    "보통 학습 데이터셋과 검증 데이터셋, 테스트 데이터셋으로 나누어 사용. 이때 검증 데이터 셋은 알고리즘을 경험해보지 못한 데이터셋으로 측정해야한다. (알고리즘을 탄 데이터의 경우 알고리즘의 일반화 역량이 사라질 수 있기 때문)\n",
    "\n",
    "학습데이터(알고리즘 학습) > 검증데이터(하이퍼파라미터 튜닝) > 테스트데이터(성능 측정)\n",
    "\n",
    "파라미터의 유형으로는 **가중치**, **하이퍼파라미터**가 있다.\n",
    "\n",
    "가중치의 경우 알고리즘 내부에서 사용되며 옵티마이저에 의해 변경되거나 역전파(backward) 과정에서 튜닝된다.\n",
    "\n",
    "하이퍼파라미터는 레이어수와 학습률 등 수동으로 변경되는 파라미터를 말할 수 있다.\n",
    "\n",
    "학습 데이터셋에서는 잘 동작하지만 검증 데이터셋이나 테스트 에서 잘 도앚ㄱ되지 않을 때 **과대적합**이라고 한다. 이와 반대의 현상을 **과소적합**이라고 한다.\n",
    "\n",
    "\n",
    "### 학습, 검증, 테스트\n",
    "\n",
    "홀드아웃 데이터셋(학습과정에 노출되지 않고 오직 성능 측정에만 사용되는 데이터셋)을 사용하는 대표적인 방법은 아래와 같다.\n",
    "\n",
    "1. 학습 데이터셋으로만 알고리즘을 학습시킴\n",
    "2. 검증 데이터셋으로 알고리즘의 하이퍼파라미터를 튜닝함\n",
    "3. 기대 성능이 달성 될 때까지 1,2 과정을 반복함\n",
    "4. 알고리즘과 하이퍼파라미터를 고정하고 테스트 데이터셋으로 성능을 평가\n",
    "\n",
    "데이터를 훈련과 검증으로 분할하고 홀드아웃 데이터셋을 적용한느 방법은 **단순 홀드아웃 검증**, **K-겹 검증**, **반복 K-겹 검증**이 있다.\n",
    "\n",
    "1. **단순 홀드아웃 검증** : 일정 비율로 테스트 데이터셋을 나눠 알고리즘과 격리시키는 방법. / 데이터가 너무 작은 경우 데이터의 대표성이 떨어질 수 있다.\n",
    "2. **K-겹 검증** : 테스트 데이터셋을 제외한 데이터를 K부분으로 나누어 K번의 반복 학습을 시키는 방법. 이때 반복을 할 때마다 검증 데이터가 되는 데이터 부분이 계속 바뀌게 된다. / 연산량이 무지하게 많다.\n",
    "3. **반복 K-겹 검증** : (?) 그냥 K 겹 검증 방식에서 데이터 순서를 바꾸는 게 추가된 듯\n",
    "\n",
    "**데이터 분할 시 주의해야할 사항**\n",
    "1. 데이터 표현력 : 대표성을 잘 나타낼 수 있도록 순서를 섞거나 층화 추출법을 사용한다. (층화 추출법이란 데이터를 각 범주로 미리 구분하여 각 범주에서 일정 비율을 데이터 요소로 선택하는 방식)\n",
    "2. 시간 민감도 : 주식과 같은 시간에 민감한 데이터의 경우 검증 데이터셋을 12월로 잡는 것이 적합하다 할 수 있다. \n",
    "3. 데이터 중복 : 중복이 되지 않도록 주의\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 전처리와 특성공학\n",
    "\n",
    "### 벡터화\n",
    "\n",
    "가장 먼저 해야할 것은 데이터를 파이토치 Tensor로 변환하기\n",
    "\n",
    "### 수치 정규화\n",
    "\n",
    "딥러닝 알고리즘에 전달하기 전 일반적으로 특성 정규화 작업을 수행한다. 이때 특성 정규화란 평균이 0이고 표준편차가 1인 데이터로 만드는 과정을 말한다.\n",
    "\n",
    "알고리즘의 성능을 더 향상하기 위해서는 아래의 데이터 특성을 갖는게 좋다.\n",
    "1. 작은 값 : 각 특성의 값은 0에서 1 사이 값\n",
    "2. 같은 범위 : 모든 특성은 같은 범위를 가져야함.\n",
    "\n",
    "### 누락 데이터 처리\n",
    "\n",
    "누락데이터에 대해서는 특성의 평균값, 최빈값, 중앙값, 최솟값, 최댓값 등으로 대체하는 것이 일반적임. 별도의 알고리즘으로 해당 누락값을 예측하기도 함.\n",
    "\n",
    "### 특성 공학\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 과대적합과 과소적합\n",
    "\n",
    "과대적합은 알고리즘의 데이터의 노이즈까지 학습했을 때 종종 발생한다. 이를 해결할 수 있는 방법을 알아보자\n",
    "\n",
    "### 더 많은 데이터 확보 \n",
    "\n",
    "데이터 양이 많아지면 작은 데이터의 패턴을 학습하지 않는다. 많은 데이터가 어려운 경우 컴퓨터 비전 분야에서는 데이터 증식을 활용하기도 한다. (데이터 증식이란 하나의 사진을 회전, 자르기, 좌우반전과 같은 방식으로 이미지를 변형해 데이터를 늘리는 기술이다.)\n",
    "\n",
    "### 네트워크 크기 줄이기\n",
    "\n",
    "일반적으로 네트워크의 크기는 보통 레이어의 수, 네트워크에서 사용하는 가중치의 수로 나타난다. 만약 과대 적합이 일어났다면 중간에 선형레이어를 제거하여 레이어를 줄이는 방법을 사용할 수 있다. (여기서 레이어를 줄인다는 것은 네트워크의 기억 용량과 기억 가능한 패턴이 줄어든다는 것을 의미한다. 즉 세세한 것을 학습하지 못하도록 하는 것이다.)\n",
    "\n",
    "### 가중치 규제 적용\n",
    "\n",
    "과대 적합을 막는 핵심 원리는 **단순한 모델 구축**이다. 위에서는 레이어를 줄였다면, 이번에는 가중치가 큰값을 가지지 않도록 규제하는 방법이 있다. **규제**란 모델의 가중치가 큰 값일 경우 모델에 불이익을 주어 네트워크 제약을 가하는 것을 의미한다.\n",
    "\n",
    "1. L1 규제 : 가중치 계수들의 절댓값 합계를 비용(cost, 이떄 비용이란 데이터의 오차 합을 의미함)에 추가한다. L1규제를 가중치의 L1 노름이라고도 한다.\n",
    "2. L2 규제 : 모든 가중치 계수들의 제곱 합을 비용에 추가한다. L2 규제를 가중치의 L2  노름이라고도 한다.\n",
    "\n",
    "optimizer의 weight_decay 매개변수를 설정하여 L2 규제를 적용할 수 있다. weight_decay 매개변수의 기본값은 0이다.\n",
    "\n",
    "### 드롭아웃\n",
    "\n",
    "드롭아웃은 딥러닝에서 가장 일반적이며 강력한 가중치 규제 기법이다. 드롭아웃이란 출력 데이터중 일정 비율 만큼을 랜덤 선정하여 출력에 0을 할당해 모델의 특정 가중치가 종속되지 않도록 만든 것이다. 보토 ㅇ드롭아웃 임계값ㅇ르 0.2 ~ 0.5사이로 주로 사용한다. 테스트과정에선 드롭아웃을 사용하지 않는다. \n",
    "nn.dropout(x, training = True)\n",
    "\n",
    "### 과소적합\n",
    "\n",
    "과소 적합의 경우 사용하는 데이터의 수를 늘리거나, 레이어수를 늘리고, 모델의 가중치, 매개변수 수를 늘리는 등 모델의 복잡도를 높인다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 머신러닝 프로젝트 워크플로\n",
    "\n",
    "### 문제 정의와 데이터셋 만들기\n",
    "\n",
    "일단 문제정의를 하기 위해서는 입력 데이터와 문제 유형이 필요하다.\n",
    "\n",
    "문제 유형을 파악하면, 어떤 종류의 아키텍처와 오차함수 및 옵티마이저를 사용할 것인지 결정하기 쉬워진다.\n",
    "\n",
    "### 모델 평가 기준\n",
    "\n",
    "모델을 평가하는 기준은 비즈니스 목표로 결정된다. 클래스의 데이터 분포가 균등한 문제에서는 ROC와 AUC를 평가 지표로 사용한다. 불균형 데이터셋에서는 정밀도와 리콜을 사용한다. 랭킹 문제를 다룰 때는 평가지표로 mAP를 사용한다. \n",
    "\n",
    "### 평가 프로토콜\n",
    "\n",
    "위에서 모델 평가 지표를 결정했다면, 데이터셋을 평가하는 방법을 결정한다. \n",
    "1. **홀드아웃 검증 셋** : 데이터가 충분히 많은 경우 일반적으로 사용되는 방법\n",
    "2. **K-겹 교차 검증** : 데이터가 제한적일 경우 사용되는 방법. \n",
    "3. **반복 K-겹 검증** : 모델의 성능을 잘 파악하고자 할 경우 사용되는 방법. K-겹 교차 검증을 지정한 횟수로 반복해서 검증하는 방법.\n",
    "\n",
    "### 데이터 준비 \n",
    "\n",
    "데이터를 벡터화 한 후 텐서로 만든다. 특성의 범위를 일정하게 조정하고 정규화 되었는지도 확인한다.\n",
    "\n",
    "### 기준 모델\n",
    "매우 간단한 모델을 기준 모델로 만든다.이 기준 모델은 기준 점수를 넘어야한다. 이때 모델에 규제 혹은 드롭아웃을 적용하지 않는다. 기준 모델을 만들 때 해야하는 3가지 선택이 있다. \n",
    "\n",
    "1. 모델의 마지막 레이어 선정\n",
    "2. 오차 함수 \n",
    "3. 최적화 : 최적의 값은 반복적인 여러 모델을 실험해보며 찾는다. 보통 Adam, RMSprop을 사용한다.\n",
    "\n",
    "**문제유형 - 활성화 함수 - 오차함수\n",
    "이진분류 - 시그모이드 - nn.CrossEntropyLoss()\n",
    "다중 클래스 분류 - 소프트맥스 - nn.CorssEntropyLoss()\n",
    "다중 레이블 분류 - 시그모이드 - nn.CrossEntropyLoss()\n",
    "회귀 - 없음 - MSE\n",
    "벡터회귀 - 없음 - MSE**\n",
    "\n",
    "### 과대적합될 정도의 모델\n",
    "기준 점수를 충분히 넘는 충분한 모델을 만든 후 기준 역량을 늘린다. \n",
    "1. 기존 아키텍처에 많은 레이어 추가\n",
    "2. 기존 레이어의 가중치 늘리기\n",
    "3. 아키텍처의 학습 데이터셋의 학습 횟수(epoch) 늘리기\n",
    "\n",
    "학습 정확도가 계속 증가하지만 검증 정확도가 증가하지 않고 정확도가 떨어지기 시작하면 학습은 중단시킨다. 이때가 과대적합이 되고 있는 것이다. 이 단계에 도달하면 가중치 규제 기법을 적용한다.\n",
    "\n",
    "### 가중치 규제 적용\n",
    "\n",
    "모델을 규제하기 위해 조정할 수 있는 부분은 \n",
    "1. 드롭아웃 추가 : 여러 레이어에 드롭아웃을 추가할 수 있기 때문에 이 경우 실험을 통해 최적의 레이어를 찾는다. 비율은 보통 0.2로 시작한다. \n",
    "2. 아키텍처 변경 : 활성화함수, 레이어수, 가중치 크기 등을 변경해본다.\n",
    "3. L1 또는 L2 규제 적용 : 둘 중 하나만\n",
    "4. 학습률 변경\n",
    "5. 특성 추가 또는 학습 데이터셋 늘리기 : 더 추가하거나 인위적으로 증식\n",
    "\n",
    "항상 주의해야할 것은 테스트 데이터가 홀드아웃 데이터인지 잘 확인한다.(알고리즘에 노출되지 않도록)\n",
    "\n",
    "만약 테스트 데이터셋에서 잘 동작하지 않고 검증, 학습에서만 잘 동작한다면 K-겹 검증을 사용하거나 반복 K-겹 검증을 사용할 수 있다.\n",
    "\n",
    "### 학습률 선정 전략\n",
    "\n",
    "torch.optim.lr_scheduler 패키지에 학습률 관련 기능이 포함되어 있다. \n",
    "1. **StepLR** : 학습 속도를 변경해주는 주기(step_size)와 학습률을 변경하는 정도(gamma)를 매개변수로 갖는다. \n",
    "\n",
    "만약 학습속도가 0.01, step_size가 10, 감마가 0.1인경우 10에폭마다 학습 속도가 감마의 곱으로 변화한다.\n",
    "처음 10개의 경우 학습률은 0.001로 변경되고 다음 10개의 에폭은 0.0001로 변경된다.\n",
    "\n",
    "(예시\n",
    "\n",
    "scheduler = StepLR(optimizer, step_size = 10, gamma = 0.1)\n",
    "\n",
    "for epoch in range(100):\n",
    "\n",
    "    scheduler.step()\n",
    "    train(...)\n",
    "    validate(...)\n",
    ")    \n",
    "\n",
    "2. **MultiStepLR** : StepLR과 유사하나 학습률이 변경되는 주기는 일정하지 않다. \n",
    "3. **ExponentialLR** : 각 에폭에 대해 감마값을 사용해 학습률의 배수로 학습률을 설정\n",
    "4. **ReduceLROpPlateau** : 일반적으로 사용되는 전략 중 하나. 학습오차, 검증 오차 또는 정확도와 같은 특정 측정 항목이 정체될 때 학습률을 변경한다. 학습률을 초깃값의 2~10배 줄이는 것이 일반적.\n",
    "\n",
    "(예시\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.1, momentum = 0.9)\n",
    "\n",
    "schduler = ReduceLROnPlateau(optimizer, 'min')\n",
    "\n",
    "for epoch in range(10):\n",
    "\n",
    "    train(...)\n",
    "    val_loss = validate(...)\n",
    "    #validate() 함수를 호출한 후 step 메서드가 호출돼야함\n",
    "    scheduler.step(val_loss)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
